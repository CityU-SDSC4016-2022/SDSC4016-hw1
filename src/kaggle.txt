Batch Size:

batch size 50  .85278
batch size 100 .85134 **
batch size 200 .86139


Number of Selected Features:

feat 10 .87227
feat 15 .86369
feat 20 .86158
feat 23 .85781
feat 26 .85134 **
feat 29 .89945


Mid Layer Size:

hdim 26 .86450
hdim 39 .86655
hdim 42 .85742
hdim 45 .86378
hdim 47 .85134 **
hdim 52 .86034


Activation Function Compare:

relu      .85134 **
rrelu     .86596
LeakyReLU .85441
relu6     1.32248
elu       .85750
selu      .86277


Activation Extra:

relu only      .85134  **
relu + norm1d  1.18196
relu + dropout .86391


Todo:

lu with norm and/or dropout
mid layer size, not linear at all, have to try one by one if have time